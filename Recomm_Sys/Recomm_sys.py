# -*- coding: utf-8 -*-
"""new_DM_Assign1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14CfjUdHMfHipbQlbfh6b6zpWyGU5jKEU
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import re
import numpy as np
from sklearn.utils.extmath import randomized_svd

df_posts = pd.read_csv('/content/drive/MyDrive/DataMining_Assign1/Posts.csv', usecols = ['Id','PostTypeId','OwnerUserId','Tags','ParentId'])
df_posts.rename(columns = {'Id':'PostId'}, inplace = True)
#df_posts.head()

df_users = pd.read_csv('/content/drive/MyDrive/DataMining_Assign1/Users.csv', usecols = ['Id','DisplayName'])
df_users.rename(columns = {'Id':'UserId'}, inplace = True)
#df_users.head()

df_tags = pd.read_csv('/content/drive/MyDrive/DataMining_Assign1/Tags.csv', usecols = ['Id','TagName','Count'])
df_tags.rename(columns = {'Id':'TagId'}, inplace = True)
#df_tags.head()

df_posts_typeID2 = df_posts.query("PostTypeId == 2")

users_answers = pd.merge(left = df_users, 
         right = df_posts_typeID2,
        how= 'inner',
        left_on='UserId',
        right_on='OwnerUserId')

#users_answers.head(5)

users_answers.drop_duplicates(['OwnerUserId','ParentId'],inplace = True)

answerer_table = users_answers.groupby(['UserId','DisplayName'])['UserId'].count().reset_index(name='AnsCount')
#answerer_table.head(10)

df_tags.sort_values(by=['Count'],ascending=False).head(3)

answerer_table.sort_values(by=['AnsCount'],ascending=False).head(3)

"""Q2"""

tags_table_with_min20Anno = df_tags[df_tags['Count'] >= 20]

answerer_table_with_min20Ans = answerer_table[answerer_table['AnsCount'] >= 20]

answerer_table_with_min20Ans_meta = pd.merge(left = answerer_table_with_min20Ans, 
         right = df_posts_typeID2,
        how= 'inner',
        left_on='UserId',
        right_on='OwnerUserId')

df2_posts= answerer_table_with_min20Ans_meta.loc[answerer_table_with_min20Ans_meta.PostTypeId==2,['OwnerUserId', 'ParentId']]
df1_posts = df_posts.loc[df_posts.PostTypeId==1,['PostId','Tags']]

answerer_table_with_min20Ans_Questions = pd.merge(left = df2_posts, 
         right = df1_posts,
        how= 'inner',
        left_on='ParentId',
        right_on='PostId')

answerer_table_with_min20Ans_Questions['Tags_cleaned'] = answerer_table_with_min20Ans_Questions['Tags'].apply(lambda x: x.replace("><",' ').replace('>','').replace('<','').split())

answerer_table_with_min20Ans_Questions_Tags = answerer_table_with_min20Ans_Questions.explode(['Tags_cleaned'])

answerer_table_with_min20Ans_Questions_Tags.head(5)

answerer_table_with_min20Ans_Questions_Tags_GroupBy = answerer_table_with_min20Ans_Questions_Tags.groupby(['OwnerUserId','Tags_cleaned']).size().reset_index(name='Counts')

answerer_table_with_min20Ans_Questions_Tags_GroupBy_min20Tags = pd.merge(left = answerer_table_with_min20Ans_Questions_Tags_GroupBy, 
                                                                         right = tags_table_with_min20Anno,
                                                                         how= 'inner',
                                                                         left_on='Tags_cleaned',
                                                                         right_on='TagName')

answerer_table_with_min20Ans_Questions_Tags_GroupBy_min20Tags.head()

answerer_table_with_min20Ans_Questions_Tags_GroupBy_min20Tags.sort_values(by = ['OwnerUserId', 'TagId'], ascending = [True, True])

df_pivot = pd.pivot_table(answerer_table_with_min20Ans_Questions_Tags_GroupBy_min20Tags, index=['OwnerUserId'], columns = ['TagId'],values=['Counts'], fill_value=0)

df_pivot.shape

"""Q3"""

arr_utility_mat = df_pivot.to_numpy()

arr_utility_rating_mat = np.zeros(arr_utility_mat.shape)

for i in range(len(arr_utility_mat)):
    for j in range(len(arr_utility_mat[0])):
        value = arr_utility_mat[i][j]
        if value < 15:
            arr_utility_rating_mat[i][j] = value/3
        else:
            arr_utility_rating_mat[i][j] = 5

#taking 85% of train into test
test_row_idx = 2000
test_col_idx = 1788

train_data = arr_utility_rating_mat.copy()

test_data = arr_utility_rating_mat[test_row_idx:,test_col_idx:]

train_data[test_row_idx:,test_col_idx:] = 0

utility_matrix = np.matrix(arr_utility_rating_mat)
utility_sum = utility_matrix.sum()
print('Utility sum = '+ str(utility_sum))

row_sum = utility_matrix.sum(axis=1, dtype='float')
high_row_sum = row_sum.max()
print('high_row_sum = '+ str(high_row_sum))

col_sum = utility_matrix.sum(axis=0, dtype='float')
high_col_sum = col_sum.max()
print('high_col_sum = '+ str(high_col_sum))

train_data_matrix = np.matrix(train_data)
train_data_sum = train_data_matrix.sum()
print('train_data_sum = '+ str(train_data_sum))

test_data_matrix = np.matrix(test_data)
test_data_sum = test_data_matrix.sum()
print('test_data_sum = '+ str(test_data_sum))

test_data_dim = test_data.shape
print('test_data_dimention = '+ str(test_data_dim))

"""Q4"""

train_data_ii = np.transpose(train_data)
test_data_ii = np.transpose(test_data)

sim = np.zeros((test_data_ii.shape[0], train_data_ii.shape[0]))

for x in range(test_data_ii.shape[0]):   
  user_sizex = train_data_ii.shape[1]    
  rowx_data = train_data_ii[x+test_col_idx]     #test_col_idx = 1788
  rx_avg  =   np.sum(rowx_data)/(user_sizex)
  numer1 = rowx_data - rx_avg
  denom1 = np.sqrt(np.sum(numer1*numer1))
  for y in range(train_data_ii.shape[0]):  
    user_sizey = train_data_ii.shape[1]    
    rowy_data = train_data_ii[y]
    ry_avg  =   np.sum(rowy_data)/(user_sizey)
    numer2 = rowy_data - ry_avg
    denom2 = np.sqrt(np.sum(numer2*numer2))

    numer = np.sum(numer1*numer2)
    denom = denom1*denom2
    
    sim[x][y] = numer/denom

sim[np.isnan(sim)] = 0
#DF_sim = pd.DataFrame(sim)
#DF_sim.to_csv("sim_ii.csv")

def n_max(arr, n):
    indices = arr.ravel().argsort()[-n:]
    indices = (np.unravel_index(i, arr.shape) for i in indices)
    return [(arr[i], i) for i in indices]

neighN = 35
sim_N = []
for i in range(sim.shape[0]):
  arr_tuples = n_max(sim[i],neighN)
  sim_N.append(arr_tuples)

sim_N_arr = np.asarray(sim_N)

top_5_ind_each_test= []
top_5_corr_each_test= []
for sim1_ind in range(sim_N_arr.shape[0]):
  sim_1_data = sim_N_arr[sim1_ind] 
  top_5_idx = np.zeros(5)   # 5 neighbours
  top_5_corr = np.zeros(5)   # 5 neighbours
  neigh = 0
  for sim2_ind in range(neighN-1, -1, -1):
    sim_2_data = sim_1_data[sim2_ind]  # decrement
    idxx = sim_2_data[1]
    corr = sim_2_data[0]
    if(idxx[0]<test_col_idx): # if the indices are less than 1788, test_col_idx
      top_5_idx[neigh] = idxx[0]
      top_5_corr[neigh] = corr
      neigh = neigh + 1
    if neigh == 5:
      top_5_ind_each_test.append(top_5_idx)
      top_5_corr_each_test.append(top_5_corr)
      break

nonzero_test_inds = np.nonzero(test_data)

capR = len(nonzero_test_inds[1])
predict_N2 = np.zeros(capR)
predict_N3 = np.zeros(capR)
predict_N5 = np.zeros(capR)

predict_N2_W = np.zeros(capR)
predict_N3_W = np.zeros(capR)
predict_N5_W = np.zeros(capR)

test_data_vec = np.zeros(capR)

for idx in range(capR):
  usr_idx = nonzero_test_inds[0][idx] + test_row_idx    #2000
  itm_idx = nonzero_test_inds[1][idx] # THis is correct logic jaggi

  five_inds = top_5_ind_each_test[itm_idx]
  five_corr = top_5_corr_each_test[itm_idx]
  r2 = 0
  r3 = 0
  r5 = 0
  s2 = 0
  s3 = 0
  s5 = 0
  mul2 = 0
  mul3 = 0
  mul5 = 0
  for i in range(0,2):
    tr2 = train_data[usr_idx][int(five_inds[i])]
    r2 = r2 + tr2

    ts2 = five_corr[i]
    mul2 = mul2 + tr2*ts2
    s2 = s2 + ts2
  for j in range(0,3):
    tr3 = train_data[usr_idx][int(five_inds[j])]
    r3 = r3 + tr3

    ts3 = five_corr[j]
    mul3 = mul3 + tr3*ts3
    s3 = s3 + ts3
  for k in range(0,5):
    tr5 = train_data[usr_idx][int(five_inds[k])]
    r5 = r5 + tr5

    ts5 = five_corr[k]
    mul5 = mul5 + tr5*ts5
    s5 = s5 + ts5
    
  predict_N2[idx] =  r2/2
  predict_N3[idx]  = r3/3
  predict_N5[idx]  = r5/5

  predict_N2_W[idx] =  r2/s2
  predict_N3_W[idx]  = r3/s3
  predict_N5_W[idx]  = r5/s5

  test_data_vec[idx] = test_data[nonzero_test_inds[0][idx]][nonzero_test_inds[1][idx]]

print(np.sqrt((np.sum(np.square(test_data_vec-predict_N2)))/capR))
print(np.sqrt((np.sum(np.square(test_data_vec-predict_N3)))/capR))
print(np.sqrt((np.sum(np.square(test_data_vec-predict_N5)))/capR))
print('-----------------------------------------------------------')
print(np.sqrt((np.sum(np.square(test_data_vec-predict_N2_W)))/capR))
print(np.sqrt((np.sum(np.square(test_data_vec-predict_N3_W)))/capR))
print(np.sqrt((np.sum(np.square(test_data_vec-predict_N5_W)))/capR))

"""User-user"""

sim_uu = np.zeros((test_data.shape[0], train_data.shape[0]))

for x in range(test_data.shape[0]):   
  user_sizex = train_data.shape[1]    
  rowx_data = train_data[x+test_row_idx]     #test_row_idx = 2000
  rx_avg  =   np.sum(rowx_data)/(user_sizex)
  numer1 = rowx_data - rx_avg
  denom1 = np.sqrt(np.sum(numer1*numer1))
  for y in range(train_data.shape[0]):  
    user_sizey = train_data.shape[1]    
    rowy_data = train_data[y]
    ry_avg  =   np.sum(rowy_data)/(user_sizey)
    numer2 = rowy_data - ry_avg
    denom2 = np.sqrt(np.sum(numer2*numer2))

    numer = np.sum(numer1*numer2)
    denom = denom1*denom2
    
    sim_uu[x][y] = numer/denom

sim_uu[np.isnan(sim_uu)] = 0

neigh_uu_N = 35
sim_uu_N = []
for i in range(sim_uu.shape[0]):
  arr_tuples = n_max(sim_uu[i],neigh_uu_N)
  sim_uu_N.append(arr_tuples)

sim_uu_N_arr = np.asarray(sim_uu_N)

uu_top_5_ind_each_test= []
uu_top_5_corr_each_test= []
for sim1_ind in range(sim_uu_N_arr.shape[0]):
  sim_1_data = sim_uu_N_arr[sim1_ind] # 10 arrays
  top_5_idx = np.zeros(5)   # 5 neighbours
  top_5_corr = np.zeros(5)   # 5 neighbours
  neigh = 0
  for sim2_ind in range(neigh_uu_N-1, -1, -1):
    sim_2_data = sim_1_data[sim2_ind]  # staring from 9
    idxx = sim_2_data[1]
    corr = sim_2_data[0]
    if(idxx[0]<test_row_idx): # if the indices are less than 2000, test_row_idx
      top_5_idx[neigh] = idxx[0]
      top_5_corr[neigh] = corr
      neigh = neigh + 1
    if neigh == 5:
      uu_top_5_ind_each_test.append(top_5_idx)
      uu_top_5_corr_each_test.append(top_5_corr)
      break

capR_uu = len(nonzero_test_inds[1])
uu_predict_N2 = np.zeros(capR_uu)
uu_predict_N3 = np.zeros(capR_uu)
uu_predict_N5 = np.zeros(capR_uu)

uu_predict_N2_W = np.zeros(capR_uu)
uu_predict_N3_W = np.zeros(capR_uu)
uu_predict_N5_W = np.zeros(capR_uu)

uu_test_data_vec = np.zeros(capR_uu)

for idx in range(capR_uu):
  usr_idx = nonzero_test_inds[0][idx] + test_col_idx    #1788
  itm_idx = nonzero_test_inds[1][idx] # THis is correct logic jaggi

  five_inds = uu_top_5_ind_each_test[itm_idx]
  five_corr = uu_top_5_corr_each_test[itm_idx]
  r2 = 0
  r3 = 0
  r5 = 0
  s2 = 0
  s3 = 0
  s5 = 0
  mul2 = 0
  mul3 = 0
  mul5 = 0
  for i in range(0,2):
    tr2 = train_data[usr_idx][int(five_inds[i])]
    r2 = r2 + tr2

    ts2 = five_corr[i]
    mul2 = mul2 + tr2*ts2
    s2 = s2 + ts2
  for j in range(0,3):
    tr3 = train_data[usr_idx][int(five_inds[j])]
    r3 = r3 + tr3

    ts3 = five_corr[j]
    mul3 = mul3 + tr3*ts3
    s3 = s3 + ts3
  for k in range(0,5):
    tr5 = train_data[usr_idx][int(five_inds[k])]
    r5 = r5 + tr5

    ts5 = five_corr[k]
    mul5 = mul5 + tr5*ts5
    s5 = s5 + ts5
    
  uu_predict_N2[idx] =  r2/2
  uu_predict_N3[idx]  = r3/3
  uu_predict_N5[idx]  = r5/5

  uu_predict_N2_W[idx] =  r2/s2
  uu_predict_N3_W[idx]  = r3/s3
  uu_predict_N5_W[idx]  = r5/s5

  uu_test_data_vec[idx] = test_data[nonzero_test_inds[0][idx]][nonzero_test_inds[1][idx]]

print(np.sqrt((np.sum(np.square(uu_test_data_vec-uu_predict_N2)))/capR_uu))
print(np.sqrt((np.sum(np.square(uu_test_data_vec-uu_predict_N3)))/capR_uu))
print(np.sqrt((np.sum(np.square(uu_test_data_vec-uu_predict_N5)))/capR_uu))
print('--------------------------------------------------------------')
print(np.sqrt((np.sum(np.square(uu_test_data_vec-uu_predict_N2_W)))/capR_uu))
print(np.sqrt((np.sum(np.square(uu_test_data_vec-uu_predict_N3_W)))/capR_uu))
print(np.sqrt((np.sum(np.square(uu_test_data_vec-uu_predict_N5_W)))/capR_uu))

"""Q5"""

def matrix_factorization(R, P, Q, K, lr, lam1=0.0, lam2=0.0):
    steps=2000
    Q = Q.T
    for step in range(steps):
        for i in range(len(R)):
            for j in range(len(R[i])):
                if R[i][j] > 0:
                    eij = R[i][j] - np.dot(P[i,:],Q[:,j])
                    for k in range(K):
                        P[i][k] = P[i][k] + lr * (2 * eij * Q[k][j] - lam1 * P[i][k])
                        Q[k][j] = Q[k][j] + lr * (2 * eij * P[i][k] - lam2 * Q[k][j])
        eR = np.dot(P,Q)
        e = 0
        for i in range(len(R)):
            for j in range(len(R[i])):
                if R[i][j] > 0:
                    e = e + pow(R[i][j] - np.dot(P[i,:],Q[:,j]), 2)
                    for k in range(K):
                        e = e + (lam1 * pow(P[i][k],2) + lam2 * pow(Q[k][j],2))
        if e < 0.001:
            break
    return P, Q.T

#run this cell for different combinations of Kmf, lam1 and lam2 as below
# lam1 = 0 and lam2 = 0 for "without regularization"

Kmf =  2    #[2,3,5]
lam1 = 0    #[0, 0.001, 0.05, 0.5]  refer the Question for each run of this cell
lam2 = 0    #[0, 0.003, 0.05, 0.75] refer the Question for each run of this cell

Pu, Sigma, QiT = randomized_svd(train_data_ii, 
                              n_components=Kmf,
                              n_iter=5,
                              random_state=None)

lr = 0.0005

nP, nQ = matrix_factorization(train_data_ii,Pu,QiT.T,Kmf,lr,lam1,lam2)
nR = np.dot(nP, nQ.T)

predict_K2 = np.zeros(capR)
test_data_vec = np.zeros(capR)

for idx in range(capR):
  usr_idx = nonzero_test_inds[0][idx] + test_row_idx    
  itm_idx = nonzero_test_inds[1][idx] + test_col_idx

  test_data_vec[idx] = test_data[nonzero_test_inds[0][idx]][nonzero_test_inds[1][idx]]

  predict_K2[idx] = nR[itm_idx][usr_idx]


print(np.sqrt((np.sum(np.square(test_data_vec-predict_K2)))/capR))

"""Q6"""

answerer_table_with_min20Ans_min20Tags_with_rate = answerer_table_with_min20Ans_Questions_Tags_GroupBy_min20Tags.copy()

answerer_table_with_min20Ans_min20Tags_with_rate['Rating'] = answerer_table_with_min20Ans_min20Tags_with_rate.apply(lambda x: x.Counts/3 if x.Counts <  15 else 5, axis = 1)

answerer_table_with_min20Ans_min20Tags_with_rate_cleaned  = answerer_table_with_min20Ans_min20Tags_with_rate.drop(columns = ['Tags_cleaned','Counts','TagName','Count'])

answerer_table_with_min20Ans_min20Tags_with_rate_cleaned.head(3)

!pip install scikit-surprise

from surprise import Dataset, Reader, accuracy
from surprise import get_dataset_dir, dump
from surprise.model_selection import train_test_split
from surprise import KNNBaseline

reader = Reader(rating_scale=(0, 5))
data = Dataset.load_from_df(answerer_table_with_min20Ans_min20Tags_with_rate_cleaned,reader)

train, test = train_test_split(data, test_size=.15)

K = 2
sim_options = {'name': 'pearson',
               'min_support': 5,
               'user_based': False}
knnbaseline = KNNBaseline(k=K,sim_options=sim_options)

knnbaseline.fit(train)
preds = knnbaseline.test(test)
accuracy.rmse(preds)

K = 3
sim_options = {'name': 'pearson',
               'min_support': 5,
               'user_based': False}
knnbaseline = KNNBaseline(k=K,sim_options=sim_options)

knnbaseline.fit(train)
preds = knnbaseline.test(test)
accuracy.rmse(preds)

K = 5
sim_options = {'name': 'pearson',
               'min_support': 5,
               'user_based': False}
knnbaseline = KNNBaseline(k=K,sim_options=sim_options)

knnbaseline.fit(train)
preds = knnbaseline.test(test)
accuracy.rmse(preds)

K = 2
sim_options = {'name': 'pearson',
               'min_support': 5,
               'user_based': True}
knnbaseline = KNNBaseline(k=K,sim_options=sim_options)

knnbaseline.fit(train)
preds = knnbaseline.test(test)
accuracy.rmse(preds)

K = 3
sim_options = {'name': 'pearson',
               'min_support': 5,
               'user_based': True}
knnbaseline = KNNBaseline(k=K,sim_options=sim_options)

knnbaseline.fit(train)
preds = knnbaseline.test(test)
accuracy.rmse(preds)

K = 5
sim_options = {'name': 'pearson',
               'min_support': 5,
               'user_based': True}
knnbaseline = KNNBaseline(k=K,sim_options=sim_options)

knnbaseline.fit(train)
preds = knnbaseline.test(test)
accuracy.rmse(preds)

from surprise import Dataset, SVD
from surprise.model_selection import GridSearchCV

param_grid = {"n_epochs": [10], "lr_all": [0.0005]}
gs = GridSearchCV(SVD, param_grid, measures=["rmse"], cv=2)
gs.fit(data)
print(gs.best_score["rmse"])

param_grid = {"n_epochs": [10], "lr_all": [0.0005]}
gs = GridSearchCV(SVD, param_grid, measures=["rmse"], cv=5)
gs.fit(data)
print(gs.best_score["rmse"])

param_grid = {"n_epochs": [10], "lr_all": [0.0005]}
gs = GridSearchCV(SVD, param_grid, measures=["rmse"], cv=10)
gs.fit(data)
print(gs.best_score["rmse"])